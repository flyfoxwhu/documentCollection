# 分布式事务
## 一、分布式事务应用场景
在微服务架构中，随着服务的逐步拆分，数据库私有已经成为共识，这也导致所面临的分布式事务问题成为微服务落地过程中一个非常难以逾越的障碍，但是目前尚没有一个完整通用的解决方案。

其实不仅仅是在微服务架构中，随着用户访问量的逐渐上涨，数据库甚至是服务的分片、分区、水平拆分、垂直拆分已经逐渐成为较为常用的提升瓶颈的解决方案，因此越来越多的原子操作变成了跨库甚至是跨服务的事务操作。最终结果是在对高性能、高扩展性，高可用性的追求的道路上，我们开始逐渐放松对一致性的追求，但是在很多场景下，尤其是账务，电商等业务中，不可避免的存在着一致性问题，使得我们不得不去探寻一种机制，用以在分布式环境中保证事务的一致性。

## 二、理论基石
随着互联化的蔓延，各种项目都逐渐向分布式服务做转换。如今微服务已经普遍存在，本地事务已经无法满足分布式的要求，由此分布式事务问题诞生。 分布式事务被称为世界性的难题，目前分布式事务存在两大理论依据：CAP定律 BASE理论。

### 1、CAP定律
这个定理的内容是指的是在一个分布式系统中、Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可得兼。

#### 一致性（C）
在分布式系统中的所有数据备份，在同一时刻是否同样的值。（等同于所有节点访问同一份最新的数据副本）

#### 可用性（A）
在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。（对数据更新具备高可用性）

数据的一致性模型可以分成以下3类：
* 强一致性：数据更新成功后，任意时刻所有副本中的数据都是一致的，一般采用同步的方式实现。
* 弱一致性：数据更新成功后，系统不承诺立即可以读到最新写入的值，也不承诺具体多久之后可以读到。
* 最终一致性：弱一致性的一种形式，数据更新成功后，系统不承诺立即可以返回最新写入的值，但是保证最终会返回上一次更新操作的值。

分布式系统数据的强一致性、弱一致性和最终一致性可以通过Quorum NRW算法分析。

#### 分区容错性（P）
以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。

### 2、BASE理论
BASE是Basically Available（基本可用）、Soft state（软状态）和 Eventually consistent（最终一致性）三个短语的缩写。BASE理论是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结， 是基于CAP定理逐步演化而来的。BASE理论的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。

#### 基本可用
基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性----注意，这绝不等价于系统不可用。比如：

（1）响应时间上的损失。正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障，查询结果的响应时间增加了1~2秒

（2）系统功能上的损失：正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面

#### 软状态
软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时

#### 最终一致性
最终一致性强调的是所有的数据副本，在经过一段时间的同步之后，最终都能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

具体可以参照下文
> https://www.infoq.cn/article/2018/08/rocketmq-4.3-release
https://www.txlcn.org/zh-cn/docs/preface.html

### 3、2PC/3PC
谈到分布式事务，首先要说的就是 2PC（two phase commit）方案，如下图所示：
![](https://i.imgur.com/kTgT6TO.png)
2PC把事务的执行分为两个阶段，第一个阶段即 prepare 阶段，这个阶段实际上就是投票阶段，协调者向参与者确认是否可以共同提交，再得到全部参与者的所有回答后，协调者向所有的参与者发布共同提交或者共同回滚的指令，用以保证事务达到一致性。

但是分布式系统中的所有通信均存在着三种状态：成功，失败，超时。其中，超时状态的存在是我们在设计分布式系统时所面对的永远的痛，2PC同样存在问题，尤其是在发送完可以提交的指令后，参与者在没有收到提交或者回滚的指令时，面对已经上锁的资源，面对已经写出去的undo或者redo日志，参与者会一时陷入手足无措的状态，为了解决这个问题，3PC应运而生。

![](https://i.imgur.com/ZgF2f8w.png)
3PC在commit之前增加了preCommit的过程，使得在参与者在收不到确认时，依然可以从容commit或者rollback，避免资源锁定太久导致浪费。但是3PC同样存在着很多问题。实现起来非常复杂，因为很难通过多次询问来解决系统间分歧问题，尤其是存在超时状态互不信任的分布式网络中，这也就是著名的拜占庭将军问题。

2PC是几乎所有分布式事务算法的基础，后续的分布式事务算法几乎都由此改进而来，其优点和缺点也比较明显
* 优点：在于已经有较为成熟的实现方案，比如XA。
* 缺点：XA是一个阻塞协议。服务在投票后需要等待协调器的决定，此时服务会阻塞并锁定资源。由于其阻塞机制和最差时间复杂度高， 因此，这种设计不能适应随着事务涉及的服务数量增加而扩展的需要，很难用于并发较高以及子事务声明周期较长 (long-running transactions) 的分布式服务中。


## 三、实现方案
### 1、实现1 Fescar
#### 介绍&框架
Fescar阿里开源，其特点是用一个事务管理器，来管理每个服务的事务，本质上是2PC（后文会解释）的一种实现。

Fescar提供了全局的事务管理器
![](https://i.imgur.com/2vOWPNq.png)

#### 原理
Fescar设计了一个**全局的排他锁**，来保证事务间的**写隔离**。全局事务的隔离性是建立在分支事务的本地隔离级别基础之上的。在数据库本地隔离级别读已提交或以上的前提下，Fescar设计了由事务协调器维护的 全局写排他锁，来保证事务间的 写隔离，将 全局事务默认定义在读未提交的隔离级别上。

我们对隔离级别的共识是：绝大部分应用在读已提交的隔离级别下工作是没有问题的。而实际上，这当中又有绝大多数的应用场景，实际上工作在读未提交的隔离级别下同样没有问题。在极端场景下，应用如果需要达到全局的读已提交，Fescar也提供了相应的机制来达到目的。默认，Fescar是工作在读未提交的隔离级别下，保证绝大多数场景的高效性。

> 需要注意的是fescar全局事务读未提交，并不是说本地事务的db数据没有正常提交，而是指全局事务二阶段commit|rollback未真正处理完（即未释放全局锁）。

总结来说：全局未提交但是本地已提交的数据，对其他全局事务是可见的当然在本地事务提交后，本地事务提交前，隔离级别是本地事务的管辖范围。

#### Fescar的全局锁
**Fescar一阶段**
* 本地（Branch）在向TC注册的时候，把本地事务需要修改的数据table+pks提交到server端申请锁，拿到全局锁后，才能提交本地事务
* 全局锁的结构：resourceId + table + pks
* 锁是存在server端branchSession中

**Fescar二阶段**
一阶段本地事务提交，db的锁释放了（for update锁），但是全局锁继续保持，
直到二阶段决议(注意释放锁的顺序)：
* 提交：TC 释放锁,通知branch提交后 (rm端异步处理)
* 回滚：TC 通知branch回滚后,释放锁（rm端同步处理 执行undo_log）

#### fescar是如何工作
**分支事务如何工作**
![](https://i.imgur.com/F5lgFCu.png)

**Fescar中RM TM TC如何工作**
![](https://i.imgur.com/3Eo2u8p.png)

#### 使用参考
[Fescar官方介绍](https://github.com/seata/seata/wiki/Home_Chinese)
[Fescar的设计理解](https://www.jianshu.com/p/4cb127b737cf)
[Seata Samples](https://github.com/seata/seata-samples) 

### 2、实现2 LCN
#### LCN介绍
LCN框架在2017年6月份发布第一个版本，从开始的1.0，已经发展到了5.0版本。LCN名称是由早期版本的LCN框架命名，在设计框架之初的1.0 ~ 2.0的版本时框架设计的步骤是如下,各取其首字母得来的LCN命名。
* 锁定事务单元（lock）
* 确认事务模块状态(confirm)
* 通知事务(notify)

5.0以后由于框架兼容了LCN、TCC、TXC三种事务模式，为了避免区分LCN模式，特此将LCN分布式事务改名为TX-LCN分布式事务框架。

#### 原理
TX-LCN由两大模块组成, TxClient、TxManager，TxClient作为模块的依赖框架，提供TX-LCN的标准支持，TxManager作为分布式事务的控制放。事务发起方或者参与反都由TxClient端来控制。
原理图:

![](https://i.imgur.com/LtnHQs4.png)

**核心步骤**
* 创建事务组：是指在事务发起方开始执行业务代码之前先调用TxManager创建事务组对象，然后拿到事务标示GroupId的过程。
* 加入事务组：添加事务组是指参与方在执行完业务方法以后，将该模块的事务信息通知给TxManager的操作。
* 通知事务组：是指在发起方执行完业务代码以后，将发起方执行结果状态通知给TxManager,TxManager将根据事务最终状态和事务组的信息来通知相应的参与模块提交或回滚事务，并返回结果给事务发起方。

#### LCN事务模式
LCN模式是通过代理Connection的方式实现对本地事务的操作，然后在由TxManager统一协调控制事务。当本地事务提交回滚或者关闭连接时将会执行假操作，该代理的连接将由LCN连接池管理。
模式特点:
* 该模式对代码的嵌入性为低。
* 该模式仅限于本地存在连接对象且可通过连接对象控制事务的模块。
* 该模式下的事务提交与回滚是由本地事务方控制，对于数据一致性上有较高的保障。
* 该模式缺陷在于代理的连接需要随事务发起方一共释放连接，增加了连接占用的时间。

#### TCC事务模式
TCC(Try-Confirm-Concel)事务机制相对于传统事务机制（X/Open XA Two-Phase-Commit），其特征在于它不依赖资源管理器(RM)对XA的支持，而是通过对（由业务系统提供的）业务逻辑的调度来实现分布式事务。TCC模型是一种补偿性事务，主要分为三个阶段：
* 初步操作 Try：完成所有业务检查，预留必须的业务资源。
* 确认操作 Confirm：真正执行的业务逻辑，不做任何业务检查，只使用Try阶段预留的业务资源。因此，只要Try操作成功，Confirm 必须能成功。另外，Confirm操作需满足幂等性，保证一笔分布式事务能且只能成功一次。
* 取消操作Cancel：释放Try阶段预留的业务资源。同样的，Cancel操作也需要满足幂等性。

如下图所示：

![](https://i.imgur.com/Lsc56r8.png)

其中，活动管理器记录了全局事务的推进状态以及各子事务的执行状态，负责推进各个子事务共同进行提交或者回滚。同时负责在子事务处理超时后不停重试，重试不成功后转手工处理，用以保证事务的最终一致性。

模式特点:
* 该模式对代码的嵌入性高，要求每个业务需要写三种步骤的操作。
* 该模式对有无本地事务控制都可以支持使用面广。
* 数据一致性控制几乎完全由开发者控制，对业务开发难度要求高。

每个子节点，要实现TCC接口，才能被管理。
**优点**：不依赖local transaction，可以管理非关系数据库库的服务

**缺点**：TCC模式多增加了一个状态，导致在业务开发过程中，复杂度上升，而且协调器与子事务的通信过程增加，状态轮转处理也更为复杂。而且，很多业务是无法补偿的，例如银行卡充值。

#### TXC事务模式
TXC模式命名来源于淘宝，实现原理是在执行SQL之前，先查询SQL的影响数据，然后保存执行的SQL快走信息和创建锁。当需要回滚的时候就采用这些记录数据回滚数据库，目前锁实现依赖redis分布式锁控制。

模式特点:
* 该模式同样对代码的嵌入性低。
* 该模式仅限于对支持SQL方式的模块支持。
* 该模式由于每次执行SQL之前需要先查询影响数据，因此相比LCN模式消耗资源与时间要多。
* 该模式不会占用数据库的连接资源。

#### 使用参考
[LCN具体介绍](https://www.txlcn.org/zh-cn/docs/background.html)
[txlcn-demo](https://github.com/codingapi/txlcn-demo)

### 3、实现3 事务消息
#### 介绍
以购物场景为例，张三购买物品，账户扣款100元的同时，需要保证在下游的会员服务中给该账户增加100积分。由于数据库私有，所以导致在实际的操作过程中会出现很多问题，比如先发送消息，可能会因为扣款失败导致账户积分无故增加，如果先执行扣款，则有可能因服务宕机，导致积分不能增加，无论是先发消息还是先执行本地事务，都有可能导致出现数据不一致的结果。

![](https://i.imgur.com/aYU9CZI.png)

事务消息的本质就是为了解决此类问题，解决本地事务执行与消息发送的原子性问题。

#### 实现框架
**传统事务消息实现**
传统事务消息实现，一种思路是依赖于 AMQP 协议用来确保消息发送成功，AMQP 模式下需要在发送在发送事务消息时进行两阶段提交，首先进行 tx_select 开启事务，然后再进行消息发送，最后进行消息的 commit 或者是 rollback。这个过程可以保证在消息发送成功的同时本地事务也一定成功执行，但事务粒度不好控制，而且会导致性能急剧下降，同时依然无法解决本地事务执行与消息发送的原子性问题。

还有另外一种思路，就是通过保证多条消息的同时可见性来保证事务一致性。但是此类消息事务实现机制更多的是用到 consume-transform-produce 场景中，其本质还是用来保证消息自身事务，并没有把外部事务包含进来。

**RocketMQ事务消息**
RocketMQ事务消息设计则主要是为了解决Producer端的消息发送与本地事务执行的原子性问题，RocketMQ的设计中broker与producer端的双向通信能力，使得broker天生可以作为一个事务协调者存在；而 RocketMQ本身提供的存储机制，则为事务消息提供了持久化能力；RocketMQ的高可用机制以及可靠消息设计，则为事务消息在系统在发生异常时，依然能够保证事务的最终一致性达成。

#### RocketMQ事务消息原理
![](https://i.imgur.com/5fhWbSV.png)

* 事务发起方首先发送prepare消息到MQ。
* 在发送prepare消息成功后执行本地事务。
* 根据本地事务执行结果返回commit或者是rollback。
* 如果消息是rollback MQ将删除该prepare消息不进行下发，如果是commit消息，MQ将会把这个消息发送给consumer端。
* 如果执行本地事务过程中，执行端挂掉，或者超时，MQ将会不停的询问其同组的其它producer来获取状态。
* Consumer端的消费成功机制有MQ保证。

**优点**：对异步操作支持友好
**缺点**：Producer端要为RMQ实现事务查询接口，导致在业务开发过程中，复杂度上升。

#### RocketMQ事务消息实现
RocketMQ事务消息在实现上充分利用了 RocketMQ本身机制，在实现零依赖的基础上，同样实现了高性能、可扩展、全异步等一系列特性。

在具体实现上，RocketMQ通过使用Half Topic以及Operation Topic两个内部队列来存储事务消息推进状态，如下图所示：

![](https://i.imgur.com/hHYfw4e.png)

其中，Half Topic对应队列中存放着prepare消息，Operation Topic对应的队列则存放了prepare message对应的commit/rollback消息，消息体中则是prepare message对应的offset，服务端通过比对两个队列的差值来找到尚未提交的超时事务，进行回查。

在具体实现上，事务消息作为普通消息的一个应用场景，在实现过程中进行了分层抽象，从而避免了对RocketMQ原有存储机制的修改，如下图所示：

![](https://i.imgur.com/MJT2R9W.png)

从用户侧来说，用户需要分别实现本地事务执行以及本地事务回查方法，因此只需关注本地事务的执行状态即可；而在 service 层，则对事务消息的两阶段提交进行了抽象，同时针对超时事务实现了回查逻辑，通过不断扫描当前事务推进状态，来不断反向请求 Producer 端获取超时事务的执行状态，在避免事务挂起的同时，也避免了 Producer 端的单点故障。而在存储层，RocketMQ 通过 Bridge 封装了与底层队列存储的相关操作，用以操作两个对应的内部队列，用户也可以依赖其它存储介质实现自己的 service，RocketMQ 会通过 ServiceProvider 加载进来。

从上述事务消息设计中可以看到，RocketMQ事务消息较好的解决了事务的最终一致性问题，事务发起方仅需要关注本地事务执行以及实现回查接口给出事务状态判定等实现，而且在上游事务峰值高时，可以通过消息队列，避免对下游服务产生过大压力。

事务消息不仅适用于上游事务对下游事务无依赖的场景，还可以与一些传统分布式事务架构相结合，而MQ的服务端作为天生的具有高可用能力的协调者，使得我们未来可以基于RocketMQ提供一站式轻量级分布式事务解决方案，用以满足各种场景下的分布式事务需求。

### 4、实现4 本地消息表
#### 介绍&原理
分布式事务 = A系统本地事务 + B系统本地事务 + 消息通知;

**准备**:
* A系统维护一张消息表log1,状态为未执行
* B系统维护2张表
* 未完成表log2
* 已完成表log3
* 消息中间件用两个topic
* topic1是A系统通知B要执行任务了
* topic2是B系统通知A已经完成任务了

**具体过程**：
![](https://i.imgur.com/iiqsGui.png)

1. 用户在A系统里领取优惠券,并往log1插入一条记录
1. 由定时任务轮询log1,发消息给B系统
1. B系统收到消息后,先检查是否在log3中执行过这条消息,没有的话插入log2表,并进行发短信,发送成功后删除log2的记录,插入log3
1. B系统发消息给A系统
1. A系统根据id删除这个消息

假设出现网络中断和系统Crash等问题时，为了继续执行事务，需要进行重试。重试方式有：定时任务恢复事务的执行，使用MQ来传递消息，MQ可以保证消息被正确消费。
**优点**：简单
**缺点**：程序会出现执行到一半的状态，重试则要求每个操作需要实现 幂等性
**注意**：分布式系统实现幂等性的时候，记得使用分布式锁，分布式锁详细介绍见文末参考文章

### 5、实现5 考拉的方案
#### 介绍&原理
考拉的方案，就是使用本地消息表，但是少了两个重要组件（MQ和 关系型数据库），写起来还是比较辛苦的。

#### 考拉方案有如下特点：
* Order表承担了消息表功能
* 服务之间使用http通信，所以碰到问题要依赖定时任务发布补单重试
* 没有使用关系型数据库，幂等性的实现比较困难。


#### 难点:
实现幂等性的要求太高，基本要求所有操作都需要实现幂等性
程序在任一一步断开，都需要重新运行起来，补单程序会很难写（简单的业务还好，复杂业务就会混乱了）

#### 改进建议：
* 服务直接使用mq通信，服务异常需要重试消费
* 使用关系型数据库，简化幂等性的实现逻辑
* 就是往上一个实现4上走

### 7、SAGA
SAGA算法于1987年提出，是一种异步的分布式事务解决方案，其理论基础在于，其假设所有事件按照顺序推进，总能达到系统的最终一致性，因此saga需要服务分别定义提交接口以及补偿接口，当某个事务分支失败时，调用其它的分支的补偿接口来进行回滚，saga的具体实现分为两种：Choreography以及Orchestration。
**Choreography**：如下图所示：
![](https://i.imgur.com/z0PkDeT.png)

这种模式下不存在协调器的概念，每个节点均对自己的上下游负责，在监听处理上游节点事件的同时，对下游节点发布事件。

**Orchestration**：存在中心节点的模式，如下图所示：
![](https://i.imgur.com/bk1WS4h.png)

该中心节点，即协调器知道整个事务的分布状态，相比于无中心节点方式，该方式有着许多优点：
* 能够避免事务之间的循环依赖关系。
* 参与者只需要执行命令 / 回复 (其实回复消息也是一种事件消息)，降低参与者的复杂性。
* 开发测试门槛低。
* 在添加新步骤时，事务复杂性保持线性，回滚更容易管理。因此大多数 saga 模型实现均采用了这种思路。

**总结**：SAGA模型的优点在于其降低了事务粒度，使得事务扩展更加容易，同时采用了异步化方式提升性能。但是其缺点在于很多时候很难定义补偿接口，回滚代价高，而且由于SAGA在执行过程中采用了先提交后补偿的思路进行操作，所以单个子事务在并发提交时的隔离性很难保证。

**使用参考**
[使用saga](https://choerodon.io/zh/docs/development-guide/backend/framework/saga/)

### 7、总结
我们先对这些实现方案进行一个总结：

| 基础原理 | 实现 | 优势 |必要前提|
| -------- | -------- | -------- |-------- |
|2PC|Seata|简单	|关系型数据库|
|2PC|TCC|不依赖关系系数据库|实现TCC接口|
|2PC|事务消息|	高性能|实现事务检查接口|
|最终一致性|本地消息表|去中心化|侵入业务，接口需要幂等性|

各个方案有自己的优劣，实际使用过程中，我们还是需要根据情况来选择不同事务方案来灵活组合。

例如存在服务模块A 、B、 C。A模块是mysql作为数据源的服务，B模块是基于redis作为数据源的服务，C模块是基于mongo作为数据源的服务。若需要解决他们的事务一致性就需要针对不同的节点采用不同的方案，并且统一协调完成分布式事务的处理。
![](https://i.imgur.com/r7oIRtd.png)

方案：将A模块采用 Seata 模式、B/C采用TCC模式就能完美解决。

## 三、具体示例
这里我们定义一个充值需求，后续我们在各个实现中看看如何为该需求实现分布式事务。
![](https://i.imgur.com/C72J46w.png)

Order和Account分别是独立的一个服务，充值完成后，要分别将订单Order设置为成功以及增加用户余额

### 1、Fescar实现充值需求
用该方案实现需求的话，就是这样的：
![](https://i.imgur.com/rfMeq4r.png)

Order和Account都接入Seata来代理事务

#### 2、TCC实现充值需求
![](https://i.imgur.com/MjdKTE5.jpg)

需要把Oder.done和Account的余额 + 操作都实现tcc接口
可以看出，这样真的很麻烦，能用本地事务的还是尽量用本地事务吧

### 3、事务消息实现充值需求
![](https://i.imgur.com/WJZ4zAz.jpg)

通过MQ，来保障Order和Acount的两个操作要么一起成功，要么一起失败。

注意一个点，假设Account的余额+失败了，这里是无法回滚Order的操作的，Account要保证自己能正确处理消息。

#### 4、本地消息实现充值需求
![](https://i.imgur.com/YuHgrO5.jpg)
通过消息表，把断开的事务继续执行下去。

### 5、考拉实现充值需求
![](https://i.imgur.com/QXTQiht.jpg)