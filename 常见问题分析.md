# 常见问题分析查看方式

## 一、rdis常见问题
### 1、发现慢查询
（1）slowlog get [N]  选型：N，可选，代表获取的日志条数
各项指标表示：
slowlog的流水号
unix时间戳
平均耗时（注意，microseconds翻译成微秒，而不是毫秒）
执行的命令和参数

（2）slowlog len 获取总共的慢查询数量

### 2、发现大对象
redis-cli -h {ip} -p {port} -a {password} --bigkeys
### 3、CPU使用情况
redis-cli -h {ip} -p {port} -a {password} --stat
### 4、持久化导致阻塞
fork阻塞: fork操作本身耗时过长，会导致主线程阻塞。通过info stats中的latest_fork_usec指标确定（单位为微秒），表示最近一次fork操作耗时，如果耗时很大，比如超过1秒，则需要做优化调整，比如不使用过大内存实例，或者规避fork缓慢的xen虚拟机。

AOF刷盘阻塞:当我们开启AOF持久化功能时，文件刷盘的方式一般采用每秒一次，后台线程每秒对AOF文件做fsync操作。当硬盘压力过大时，fsync操作需要等待，直到写入完成。如果主线程发现距离上一次的fsync成功超过2秒，为了数据安全性它会阻塞直到后台线程执行fsync操作完成。这种阻塞行为主要是硬盘压力引起。后台日志会出现如下信息：
Asynchronous AOF fsync is taking too long (disk is busy). Writing the AOFbuffer without waiting for fsync to complete, this may slow down Redis.


### 5、网络问题
#### （1）Redis连接拒绝：
Redis通过maxclients参数控制客户端最大连接数，默认10000。查看info stats的rejected_connections统计指标展示被拒绝的数量。客户端访问尽量采用长连接或者连接池方式。
查看某个应用的redis连接数：
	```for x in `ps aux | grep {appName} | grep admin|grep java | awk '{print $2}'`;do netstat -natp | grep {redisPort} | grep $x -c;done```

#### （2）backlog队列溢出
系统默认backlog为128
优化：使用echo 512>/proc/sys/net/core/somaxconn修改系统默认参数
队列溢出统计：netstat-s|grepoverflowed，查看是否有持续增长的连接拒绝情况。
#### （3）网络延时:
网络延时统计: redis-cli -h {host} -p {port} --latency
分别统计：最小值、最大值、平均值、采样次数
网络延时一般发生在跨机房部署
#### （4）网卡软中断
单个网卡队列只能使用一个CPU，高并发下网卡数据集中在一个CPU下，导致无法利用多核CPU。网卡软中断瓶颈一般出现在网络高流量吞吐场景，top的si指标过高。

### 6、redis监控数据采集
#### （1）redis存活监控:
redis本地监控agent使用ping，如果指定时间返回PONG表示存活，否则redis不能响应请求，可能阻塞或死亡。当返回值不为1时，redis挂了
```redis-cli -c -h {host} -p {port} -a {password} ping | grep -c PONG```

#### （2）连接个数：
客户端连接个数，如果连接数过高，影响redis吞吐量

```redis-cli -c -h {host} -p {port} -a {password} info | grep -w "connected_clients" | awk -F":" '{print $2}'```

#### （3）拒绝的连接个数：

```redis-cli -c -h {host} -p {port} -a {password} info | grep -w rejected_connections```


#### （4）新创建连接个数: 
如果新创建连接过多，过度地创建和销毁连接对性能有影响，说明短连接严重或连接池使用有问题，告警。

```redis-cli -c -h {host} -p {port} -a {password} info | grep -w total_connections_received```


#### （5）list阻塞调用被阻塞的连接个数:

```redis-cli -c -h {host} -p {port} -a {password} info | grep -w blocked_clients```


#### （6）redis分配的内存大小: 
redis真实使用内存，不包含内存碎片

```redis-cli -c -h {host} -p {port} -a {password} info | grep -w used_memory```


#### （7）redis进程使用内存大小: 
进程实际使用的物理内存大小，包含内存碎片；如果rss过大导致内部碎片大，内存资源浪费，和fork的耗时和cow内存都会增大。

```redis-cli -c -h {host} -p {port} -a {password} info | grep -w used_memory_rss```


#### （8）redis内存碎片率 : 
使用内存大小/分配的内存大小的值，碎片率过大，导致内存资源浪费，不设置告警。小于1，表示redis已使用swap分区，需要告警

```redis-cli -c -h {host} -p {port} -a {password} info | grep -w mem_fragmentation_ratio```


#### （9）键个数 (keys): 
redis实例包含的键个数。单实例键个数过大，可能导致过期键的回收不及时

```redis-cli -c -h {host} -p {port} -a {password} info | grep -w keys | awk -F':' '{print $2}' | awk -F',' '{print $1}' | awk -F'=' '{print $2}' ```

#### （10）redis处理的命令数: 
监控采集周期内的平均qps

```redis-cli -c -h {host} -p {port} -a {password} info | grep -w total_commands_processed| awk -F':' '{print $2}'```

#### （11）redis当前的qps:
redis内部较实时的每秒执行的命令数

```redis-cli -c -h {host} -p {port} -a {password} info | grep -w instantaneous_ops_per_sec | awk -F':' '{print $2}' ```


#### （12）请求键被命中次数: 
redis请求键被命中的次数

```redis-cli -c -h {host} -p {port} -a {password} info | grep -w keyspace_hits | awk -F':' '{print $2}' ```


#### （13）请求键未被命中次数: 
redis请求键未被命中的次数

``` redis-cli -c -h {host} -p {port} -a {password} info | grep -w keyspace_misses ```


#### （14）请求键的命中率:
使用请求键被命中次数/(请求键被命中次数+请求键未被命中次数)计算所得，命中率低于50%告警

#### （15）最近一次fork阻塞的微秒数: 
最近一次Fork操作阻塞redis进程的耗时数，单位微秒。

```redis-cli -c -h {host} -p {port} -a {password} monitor```
#### （16）redis执行的命令监控:
```redis-cli -c -h {host} -p {port} -a {password} info | grep -w latest_fork_usec```

## 二、应用系统常见问题查看
### 1、CPU问题
首先，通过top查看是哪个进程存在问题
然后，列出存在问题的进程列表,找出异常线程
```top -H -p <java 进程pid> ```
接着，将异常线程的jvm栈dump下来，或直接查看线程的所有堆栈信息
```
dump操作：jstack -l <其中一个线程PID> >> stack.log
查看堆栈：jstack <其中一个线程PID>
```
最后，对线程的dump进行分析

#### 分析原则
>线程Dump需要结合代码阅读推理相互推导，造成Bug的根源往往会在调用栈上直接体现,一定格外注意线程当前调用之前的所有调用。

#### dump文件中的关键词和对应线程状态

| 关键词 | 线程状态 | 描述|
| ------ | ------ | ------ | 
| DeadLock | 死锁 | 如果进入同步方法或同步代码块，没有获取到锁，则会进入该状态|
| Runnable | 执行中| 当调用thread.start()后，线程变成为Runnable状态。只要得到CPU，就可以执行；|
|wating on condition|等待资源|等待某个资源或条件发生来唤醒自己。具体需要结合jstacktrace来分析，比如线程正在sleep，网络读写繁忙而等待|
|wating on monitor entry|等待获取监视器|如果在连续几次输出线程堆栈信息都存在于同一个或多个线程上时，则说明系统中有锁竞争激烈，死锁，或锁饿死的想象|
|Suspend|暂停||
|Object.wait()或TIMED_WAITING|对象等待中||
|Blocked|阻塞||
|Parked|停止||



#### 对于jstack的dump分析思路：

1、如果某个相同的call stack经常出现， 我们有80%的以上的理由确定这个代码存在性能问题（读网络的部分除外）；
2、如果相同的call stack出现在同一个线程上（tid）上， 我们很很大理由相信， 这段代码可能存在较多的循环或者死循环；
3、如果某call stack经常出现， 并且里面带有lock，请检查一下这个lock的产生的原因， 可能是全局lock造成了性能问题；
4、在一个不大压力的群集里（w<2）， 我们是很少拿到带有业务代码的stack的， 并且一般在一个完整stack中， 最多只有1-2业务代码的stack，
5、如果经常出现， 一定要检查代码， 是否出现性能问题。
6、如果你怀疑有dead lock问题， 那么请把所有的lock id找出来，看看是不是出现重复的lock id。


ps:为了方便定位线程产生的问题，在创建线程池的时候，尽量避免使用Executors，建议使用手动创建线程池的方式：
```
/**
  * 默认5条线程（默认数量，即最少数量），
  * 最大20线程（指定了线程池中的最大线程数量），
  * 空闲时间0秒（当线程池梳理超过核心数量时，多余的空闲时间的存活时间，即超过核心线程数量的空闲线程，在多长时间内，会被销毁），
   * 等待队列长度1024，
  * 线程名称[MXR-Task-%d],方便回溯，
  * 拒绝策略：当任务队列已满，抛出RejectedExecutionException
  * 异常。
  */
 private static ThreadPoolExecutor threadPool = new ThreadPoolExecutor(5, 20, 0L,
      TimeUnit.MILLISECONDS, new LinkedBlockingQueue<>(1024)
     , new ThreadFactoryBuilder().setNameFormat("My-Task-%d").build()
     , new AbortPolicy()
 );
```

### 2、内存问题
#### (1)、jmap查看jvm内存情况
查看整个JVM内存状态 
jmap -heap [pid]
要注意的是在使用CMS GC 情况下，jmap -heap的执行有可能会导致JAVA 进程挂起

查看JVM堆中对象详细占用情况
jmap -histo [pid]

导出整个JVM 中内存信息
jmap -dump:format=b,file=文件名 [pid]

jhat是sun 1.6及以上版本中自带的一个用于分析JVM 堆DUMP 文件的工具，基于此工具可分析JVM HEAP 中对象的内存占用情况
jhat -J-Xmx1024M [file]
执行后等待console 中输入start HTTP server on port 7000 即可使用浏览器访问 IP：7000

Java VisualVM
JDK自1.6以后提供了可视化工具jvisualvm，mac上直接运行jvisualvm就会有对应的操作界面
![avatar](https://img.maihaoche.com/3F5F7208-A2FC-466A-BFDB-261E2DECE38F.png)
启动 Java VisualVM 后可以看到窗口左侧 应用程序栏中有本地、远程 、快照三个项目。本地下显示的是在localhost运行的Java程序的资源占用情况,如果本地有Java程序在运行的话启动Java VisualVM 即可看到相应的程序名，点击程序名打开相应的资源监控菜单，以图形的形式列出程序所占用的CPU、Heap、PermGen、类、线程的 统计信息。远程项下列出的远程主机上的Java程序的资源占用情况，但需要在远程主机上运行jstatd守护程序。

 ps：内存不足的原因不一定是因为java进程导致的，有可能是用户的不正当操作，如：通过vi查看一个比较大的文件直接导致机器内存消耗尽，应用挂掉
 
#### (2)jstat命令查看jvm的GC情况

*类加载统计:jstat -class [pid]*
![avatar](https://img.maihaoche.com/2428DF6C-CDC6-460A-A533-0F742379FE98.png)
>Loaded:加载class的数量
>Bytes：所占用空间大小
>Unloaded：未加载数量
>Bytes:未加载占用空间
>Time：时间
 
*垃圾回收统计:jstat -gc [pid]*
![avatar](https://img.maihaoche.com/D300698D-616E-43EF-8CF0-D0B0ACF73415.png)
>S0C：第一个幸存区的大小
S1C：第二个幸存区的大小
S0U：第一个幸存区的使用大小
S1U：第二个幸存区的使用大小
EC：伊甸园区的大小
EU：伊甸园区的使用大小
OC：老年代大小
OU：老年代使用大小
MC：方法区大小
MU：方法区使用大小
CCSC:压缩类空间大小
CCSU:压缩类空间使用大小
YGC：年轻代垃圾回收次数
YGCT：年轻代垃圾回收消耗时间
FGC：老年代垃圾回收次数
FGCT：老年代垃圾回收消耗时间
GCT：垃圾回收消耗总时间

*总结垃圾回收统计:jstat -gcutil [pid]*
![avatar](https://img.maihaoche.com/AFC36E70-A22F-4037-9911-EC3CB6263479.png)
>S0：幸存1区当前使用比例
S1：幸存2区当前使用比例
E：伊甸园区使用比例
O：老年代使用比例
M：元数据区使用比例
CCS：压缩使用比例
YGC：年轻代垃圾回收次数
FGC：老年代垃圾回收次数
FGCT：老年代垃圾回收消耗时间
GCT：垃圾回收消耗总时间

*堆内存统计:jstat -gccapacity [pid]*
![avatar](https://img.maihaoche.com/A7DE6C3B-7EF9-4E68-B7C8-351932C2026F.png)
>NGCMN：新生代最小容量
NGCMX：新生代最大容量
NGC：当前新生代容量
S0C：第一个幸存区大小
S1C：第二个幸存区的大小
EC：伊甸园区的大小
OGCMN：老年代最小容量
OGCMX：老年代最大容量
OGC：当前老年代大小
OC:当前老年代大小
MCMN:最小元数据容量
MCMX：最大元数据容量
MC：当前元数据空间大小
CCSMN：最小压缩类空间大小
CCSMX：最大压缩类空间大小
CCSC：当前压缩类空间大小
YGC：年轻代gc次数
FGC：老年代GC次数

对应的还可以单独查看新生代、老年代、元数据空间的gc和内存统计
新生代垃圾回收统计：jstat -gcnew [pid]
新生代内存统计：jstat -gcnewcapacity [pid]
老年代垃圾回收统计：jstat -gcold [pid]
老年代内存统计：jstat -gcoldcapacity [pid]
元数据空间统计：jstat -gcmetacapacity [pid]

#### （3）常见内存问题
##### coredump：
排查思路：一般情况下，进程coredump的时候都会留下coredump文件，coredump文件的存储位置配置在/proc/sys/kernel/core_pattern文件下。并且jvm本身也会生成一个crash报告文件，该文件的可以大概的反映出一个当时的情况，但是coredump文件能获取的信息更多。可以使用gdb工具来进行调试coredump文件，找到具体原因。

##### OOM（out of memory）：
oom就是内存不足有以下三种：
* Exception in thread "main" java.lang.OutOfMemoryError: unable to create new native thread---没有足够的内存空间为该线程分配java栈。
解决办法：很多资料说可以通过调整Xss参数可解决问题，事实上系统采用的延迟分配，所有系统并不会给每个线程都分配Xss的真实内存，是按需分配的。
出现这种情况至少95%情况是因为使用线程池（ExecutorService），忘记调用shutdown了，还有少部分情况可能是系统参数配置有问题，比如 max_threads、 max_user_processes过小。

* Exception in thread "main" java.lang.OutOfMemoryError: Java heap space---堆的内存占用已经达到-Xmx设置的最大值。
解决办法：调整-Xmx的值，或者存在内存泄漏，按照内存泄漏的思路查找。

* Caused by: java.lang.OutOfMemoryError: PermGen space---方法区的内存占用已经达到-XX:MaxPermSize设置的最大值。
解决办法：调整-XX:MaxPermSize的值。

##### StackOverflow：
Exception in thread "main" java.lang.StackOverflowError --- 线程栈需要的内存大于Xss值。
解决办法：调整Xss的值。

##### 堆内内存泄漏：
查看gc情况是否正常，堆内内存泄漏总是和gc异常相伴随的。
jmap -dump:live,file=mem.map pid把内存dump下来。
通过mat(memory analyzer)分析内存对象及调用链，发现无法回收的对象。

##### 堆外内存泄漏：
思路：堆外内存一般分为使用unsafe或者ByteBuffer申请的和使用native方式申请的。比如对于unsafe典型应用场景就是Netty，而对于native方式典型应用场景是解压包（ZipFile），笔者遇到的堆外内存泄漏90%都跟这两者相关，当然还有其他情况，比如直接使用 JavaCPP申请堆外内存（底层就是native方式）。对于堆外内存泄漏一般gperftools+btrace这组组合工具基本上都能搞定，如果不行的话，可能就需要系统底层工具了，比如 strace等。
另外，虽然堆外内存不属于堆内，但是其引用在堆内；有时候在直接查看堆外不方便或者没有结论时，可以查看堆内是否有异常对象。


### 3、IO问题

iostat主要用于监控系统设备的IO负载情况，iostat首次运行时显示自系统启动开始的各项统计信息，之后运行iostat将显示自上次运行该命令以后的统计信息。一般会重点关注iowait值，表示CPU用于等待io请求的完成时间。可以通过指定统计的次数和时间来获得所需的统计信息。

	iostat -d -k 1 10 查看TPS和吞吐量信息(磁盘读写速度单位为KB)

	iostat -d -m 2 查看TPS和吞吐量信息(磁盘读写速度单位为MB)

	iostat -d -x -k 1 10 查看设备使用率（%util）、响应时间（await）
 
	iostat -c 1 10 查看cpu状态
	
	
一般会主要看以下一些参数
* 1、iowait% 表示CPU等待IO时间占整个CPU周期的百分比，如果iowait值超过50%，或者明显大于%system、%user以及%idle，表示IO可能存在问题。 
* 2、avgqu-sz 表示磁盘IO队列长度，即IO等待个数
* 3、await 表示每次IO请求等待时间，包括等待时间和处理时间 
* 4、svctm 表示每次IO请求处理的时间(毫秒为单位)
* 5、%util 表示磁盘忙碌情况，一般该值超过80%表示该磁盘可能处于繁忙状态。

### 4、负载过高问题的分析思路
首先，运行top命令查看负载：
load average的三个值分别代表 1分钟、5分钟、15分钟前到现在的平均值如果这三个值从左到右越来越高代表服务器平均负载在下降，相反代表服务器的负载在上升,如下图：
![avatar](https://img.maihaoche.com/4D66D514-B974-4D96-8EB6-23512A274A91.png)

其次，看CPU、Mem项判断是cpu负载还是内存消耗太大或者是IO问题，一般认为平均负载数超过cpu的核数为cpu高负载；如果使用swap则内存为高消耗；在％Cpu(s) 一行中 wa这个数据较高表示CPU中出现严重等待问题，可能导致的原因就包括 读写磁盘I/O造成的。具体的CPU、IO、内存问题查看方式可以参照上文提到对应问题的处理逻辑。
接着，通过df -l查看磁盘使用率是否正常；

还有一些外部原因导致服务器负载过高，如：
1、应用访问量很大，超过服务器能承载的能力
2、服务器中病毒，需要对服务器盘查
3、受到ddos攻击等
这些问题需要通过服务监控来观察预防。

### 5、频繁GC问题或内存溢出问题
1、使用jps查看线程ID

2、使用jstat -gc [pid] 250 20 查看gc情况

3、使用jstat -gccause：额外输出上次GC原因

4、使用jmap -dump:format=b,file=heapDump.hprof [pid]生成堆转储文件

5、使用jhat或者可视化工具（MAT 、IBM HeapAnalyzer）分析堆情况。

6、结合代码解决内存溢出或泄露问题。


## 三、数据库常见问题查看

### 1、数据库连接异常
数据库连接异常一般有以下三种原因：
a、数据库的连接达到它的最大连接数
b、应用的数据库连接池达到最大连接数
c、数据库的连接信息发生变化，应用配置未及时更新

首先，看数据库本身连接是否已经用完
```
先看数据库当前设置的最大连接数 show variables like '%max_connections%';
在查看数据库连接数、并发数相关信息
show status like 'Threads%';
显示结果中：
Threads_connected 指的是打开的连接数
Threads_running 指的是激活的连接数 
如果打开的连接数和数据库设置的最大连接数差不多就是数据库本身连接存在问题
```

如果数据本身连接存在问题，可以通过查看当前数据库的具体连接情况来看存在哪些异常连接：
```查看数据库的具体连接情况：SELECT count(*) AS count,USER,db,SUBSTRING_INDEX(HOST, ':', 1) AS ip FROM information_schema. PROCESSLIST GROUP BY USER,db,ip ORDER BY count desc;```

如果数据库的连接正常，就要看应用的数据库连接情况，如果应用的数据库连接数据达到连接池的最大连接数，要么就是并发量足够大需要调整连接池的最大连接数，要么就是连接未及时释放
 
```查看应用的数据库连接情况 ：for x in `ps aux | grep {appName} | grep admin|grep java | awk '{print $2}'`;do netstat -natp | grep {dbPort} | grep $x -c ;done```

### 2、数据库CPU异常

占用CPU过高，可以做如下考虑：
```
1）一般来讲，排除高并发的因素，还是要找到导致你CPU过高的哪几条在执行的SQL，show processlist语句，查找负荷最重的SQL语句，优化该SQL，比如适当建立某字段的索引；
2）打开慢查询日志，将那些执行时间过长且占用资源过多的SQL拿来进行explain分析，导致CPU过高，多数是GroupBy、OrderBy排序问题所导致，然后慢慢进行优化改进。比如优化insert语句、优化group by语句、优化order by语句、优化join语句等等；
3）优化文件及索引；
4）考虑是否是锁问题；
5）调整一些MySQL Server参数，比如key_buffer_size、table_cache、innodb_buffer_pool_size、innodb_log_file_size等等；
6）可能由于内存泄露导致数据库CPU高
7）在多用户高并发的情况下，任何系统都会hold不住的，所以，使用缓存是必须的，可以使用memcached或者redis缓存；
8）mysql的sql语句睡眠连接超时时间设置问题（wait_timeout）
```

## 三、dubbo常见问题查看

1、如果服务注册不上
(1) 检查dubbo的jar包有没有在classpath中，以及有没有重复的jar包
(2) 检查有没有重复的dubbo.properties配置文件
(3) 检查暴露服务的spring配置有没有加载
(4) 检查beanId或beanName有没有重复
(5) 查看有没有错误日志
(6) 在服务提供者机器上测试与注册中心的网络是否通：
```telnet [注册中心IP] 9090```
(7) 检查与注册中心的连接是否存在：
```netstat -anp | grep [注册中心IP]```
(8) 开启远程调试：
在dubbo源码的DefaultRegistryService的registerService()方法中设置断点调试

2、RpcException: No provider available for remote service异常
1检查连接的注册中心是否正确
2)到注册中心查看相应的服务提供者是否存在
3)检查服务提供者是否正常运行

3、出现调用超时com.alibaba.dubbo.remoting.TimeoutException异常怎么办？
(1) 一种情况是服务请求超时.
通常是业务处理太慢，可在服务提供方执行：jstack [PID] > jstack.log 分析线程都卡在哪个方法调用上，这里就是慢的原因。
如果不能调优性能，请将timeout设大

(2) 二大类的情况是调用的版本不对
在上面我们已经说了具体的版本问题，如果你调用的对方版本不对的话，就相当于你的消费者没有提供者。所以会出现超时，此时只需要把版本对应好即可。

(3)提供者的服务被禁止
 这是一种人为的控制，通过监控中心我们可以对具体的服务，以及它的权重进行控制，当我将一个具体的服务禁止之后消费者就调不到相关的服务，此时就会出现超时的问题。取消禁止即可。注意这里有一定时间的缓存，实际操作的时候应该注意。     

（4）服务保护
考虑服务的dubbo线程池类型（fix线程池的话考虑线程池大小）、数据库连接池、dubbo连接数限制是否都合适。

（5）注册中心的分组group和服务的不同实现group
service和reference上也可以配置group，这个用于区分同一个接口的不同实现，只有在reference上指定与service相同的group才会被发现。

4、出现hessian序列化失败com.caucho.hessian.client.HessianRuntimeException？
1)检查服务方法的传入传出参数是否实现Serializable接口

2)检查服务方法的传入传出参数是否继承了Number,Date,ArrayList,HashMap等hessian特殊化处理的类


5、出现Configuration problem: Unable to locate Spring NamespaceHandler for XML schema namespace 
表示spring找不到dubbo配置的解析处理器。
通常是Dubbo的jar没有引入，请加入对Dubbo的依赖，或者是ClassLoader隔离，看是否有使用osgi或其它热加载机制

6、出现"消息发送失败"异常
通常是接口方法的传入传出参数未实现Serializable接口。

7、项目依赖的三方库与Dubbo所依赖的版本冲突
如果是IDEA上安装插件maven helper查看冲突，解决冲突

8、java.util.concurrent.RejectedExecutionException或者Thread pool exhausted
RejectedExecutionException表示线程池已经达到最大值，并且没有空闲连，拒绝执行了一些任务。Thread pool exhausted通常是min和max不一样大时，表示当前已创建的连接用完，进行了一次扩充，创建了新线程，但不影响运行。
原因可能是连接池不够用，请调整dubbo.properites中的：

``` 
//设成一样大，减少线程池收缩开销
dubbo.service.min.thread.pool.size=200
dubbo.service.max.thread.pool.size=200
```

如果线程池已经有200，还不够，通常是业务处理占用线程时间过长，
需优化业务，可通过运行：
jstack [pid] > jstack.txt
分析当前大多数线程都在干什么，从而分析出哪个地方是瓶颈，
比如，如果大部分线程都在处理SQL，可能是数据库连接不够，或数据源配置错误，或SQL没走索引等。

9、com.alibaba.dubbo.registry.internal.rpc.exception.RpcLocalExceptionIoTargetIsNotConnected异常
1) 检查注册中心是否开启白名单功能，如果开启，当IP不在白名单列表中，注册中心将拒绝连接。
2) 检查端口是否正确，注册中心有两个端口，一个为控制台HTTP端口，用于管理员查看数据，一个为程序注册服务用的TCP端口。

10、Remote server returns error: [6], Got invocation exception异常
此异常表示Dubbo框架调用服务提供者的实现方法失败，并且不是方法本身的业务异常。通常是服务消费者和服务提供者的API签名不一致引起，或者提供方比消费方少此函数。一般是服务增加方法，或修改了方法签名，而双方用的服务API的jar包不一致。

11、服务提供者没挂，但在注册中心里看不到怎么办？
首先，确认服务提供者是否连接了正确的注册中心，不只是检查配置中的注册中心地址，而且要检查实际的网络连接。
其次，看服务提供者是否非常繁忙，比如压力测试，以至于没有CPU片段向注册中心发送心跳，这种情况，减小压力，将自动恢复。

12、dubbo monitor异常ERROR monitor.StatLog -拒绝连接 java.net.ConnectException:拒绝连接
监控中心不可用，发送统计信息失败，不影响调用，但将丢失统计信息。